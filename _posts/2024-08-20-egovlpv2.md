---
title: 'EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone'
date: 2024-08-25
permalink: /posts/2024/08/egovlpv2/
tags:
  - Egocentric Video Understanding
  - Paper Understanding
---

视频语言预训练（VLP）由于其可以推广到各种视觉和语言任务中的能力而变得十分重要。然而，现有的第一视角VLP框架利用单独的视频和语言编码器，并且仅在微调期间学习特定于任务的跨模态信息，从而限制了统一系统的开发。在这项工作中，作者提出了第二代第一视角视频语言预训练（EgoVLPv2）模型，这是对上一代的重大改进，通过将跨模态融合直接纳入视频和语言主干。EgoVLPv2在预训练期间学习强视频-文本表示，并重用跨模态注意力模块，以灵活高效的方式支持不同的下游任务，降低微调成本。此外，相较于堆叠额外的融合特定的层的操作，作者提出的融合骨干网络策略是更轻量级和计算更加高效的。大量实验证明了EgoVLPv2的有效性，通过在所有下游任务的基线算法上实现了一致的最先进的性能。

> 题目可翻译为——EgoVLPv2：在骨干网络融合上预训练的第一视角视频语言模型

*声明*：本博客内容用于自我学习使用，笔记内容与想法均来源于原始论文以及先验知识体系。

📡 [论文地址](https://arxiv.org/pdf/2307.05463.pdf)/[项目页面](https://shramanpramanick.github.io/EgoVLPv2/)

阅前浏览——[EgoVLP](https://arxiv.org/abs/2206.01670)

## 目录

- [解决问题](#1-解决问题)
- [关键思路](#2-关键思路)
- [摘要](#3-摘要)
- [介绍](#4-介绍)
- [相关工作](#5-相关工作)
- [方法](#6-方法)
- [实验](#7-实验)
- [结论和未来畅想](#8-结论)
- [附录](#9-附录)

---

### 1. 解决问题

新的一种第一视角视频语言预训练网络框架，比起之前的EgoVLP来说它可以整合进行更多的下游任务，而且在参数和效率上都有方方面面的提升。

---

### 2. 关键思路

利用门控机制将交叉注意力应用在骨干网络中，并在骨干网络中使用融合

---

### 3. 摘要

视频语言预训练（VLP）由于其可以推广到各种视觉和语言任务中的能力而变得十分重要。然而，现有的第一视角VLP框架利用单独的视频和语言编码器，并且仅在微调期间学习特定于任务的跨模态信息，从而限制了统一系统的开发。在这项工作中，作者提出了第二代第一视角视频语言预训练（EgoVLPv2）模型，这是对上一代的重大改进，通过将跨模态融合直接纳入视频和语言主干。EgoVLPv2在预训练期间学习强视频-文本表示，并重用跨模态注意力模块，以灵活高效的方式支持不同的下游任务，降低微调成本。此外，相较于堆叠额外的融合特定的层的操作，作者提出的融合骨干网络策略是更轻量级和计算更加高效的。大量实验证明了EgoVLPv2的有效性，通过在所有下游任务的基线算法上实现了一致的最先进的性能。

---

### 4. 介绍

视频语言预训练任务（VLP）现阶段是各种视频文本任务的实际解决方案，例如视频文本检索任务、视觉问答任务VQA、零样本识别任务和视频时序定位任务（根据文本定位视频中对应时段，预测出起始终止时间）。现有的视频语言数据集一般分为两类：第三人称视角和第一人称视角（自我中心）。两者之间的原因使得无法互相兼顾彼此领域之中的性能表现，然而，大规模数据集Ego4D是有助于释放第一视角VLP模型的完整潜力。当前第一视角VLP方法通常是通过预训练单独的视频编码器和语言编码器，然后针对下游具体的任务在上面进行微调处理，进而学习到特定于具体任务的跨模态信息，这种方式很大程度上限制了第一视角VLP算法模型框架的发展。此外，在多模态的下游任务中缺乏比较强大的零样本性能，这个问题通常是通过在视频和文本编码器的顶部堆叠融合层进行信息整合，或者使用共享的视频语言架构来解决。然而这些方法引入了大量的融合参数，并且所得到的编码器不能直接应用于单独一种模态（仅视频）的任务。在这项工作中，作者团队提出了一个第一视角VLP算法模型框架（EgoVLPv2），通过将跨模态融合直接纳入视频和语言的主干网络中来，这是对上一代的显著改进。作者团队的方法通过以下方式改进了现有的VLP框架：

- 与堆叠的融合特定Transformer层或共享编码器相比，这个算法模型具有更少的融合参数，需要更少的GPU存储、计算资源和训练时间；
- 通过使用门控机制打开和关闭交叉注意融合，具有在双编码器和融合编码器之间切换的灵活性;
- （iii）适用于单模态和多模态任务。

将跨模态融合直接应用在主干网络中有助于统一应用广泛的基于双路编码器这种方式和基于融合编码器这种方式的各类下游任务。具体的一种使用场景例如EgoVLPv2的“switching”能力使作者团队能够利用相同的预训练编码器进行快速检索和定位任务，按照以往的方式则分别需要用到双路编码器和融合编码器。此外，与现有的第一视角VLP算法框架在微调期间学习特定任务的融合参数相比，EgoVLPv2在不同的任务中重用预先训练的交叉注意模块，显著降低了微调成本。这使作者团队能够引入以查询为中心的视频总结作为下游任务，这最近也在很多工作中有引用和体现。注释数据的稀缺，这是一个基于端到端训练适当大小的模型的一个瓶颈难题，目前唯一可用的第一视角数据集**QFVS**（***作者说这个是唯一的？？？***）仅提供了135个视频查询训练样本对儿。EgoVLPv2在QFVS上实现了新的最先进的结果，并且在基线上有不错的效果。

总的来说，贡献如下：

1. 作者团队提出EgoVLPv2，第二代[EgoVLP](https://arxiv.org/abs/2206.01670)，在骨干网络中进行跨模态融合操作，在第一视角VLP方面向前迈进了一步。作者团队提出的框架可以在双编码器和融合编码器之间切换，相较于学习额外的融合特定Transformer层操作，作者团队只需要45%的计算（GMACs）。

> 🦻友情提示：作者团队这里使用的GMAC作为计算指标？对于FLOPS、GMACs、GFLOPs等，大家容易混淆，具体有如下区别：FLOPS(Floating Point Operations Per Second)：每秒浮点运算次数，是一个衡量**硬件运算速度的评价指标**。对于GFLOPS指硬件设备每秒可以进行十亿（10^9）次浮点运算。FLOPs（Floating Point Operations）：浮点运算次数，这里看到最后的s是小写的，用来衡量模型**计算复杂度**，常用来做神经网络模型速度的间接衡量标准。GFLOPs指模型需要进行十亿（10^9）次浮点运算。MACs(Multiply–Accumulate Operations):乘加累积操作数。通常情况下，1 MACs包含一个乘法操作与一个加法操作，大约包含2 FLOPs。作者这里用到的GMACs指模型需要进行十亿次（10^9）乘加运算。通常情况下，GMACs与GFLOPs之间存在一个大约1:2的比例关系，因为每个乘加运算需要进行两个浮点运算（一个乘法和一个加法）。但是这个比例关系取决于具体的计算任务和计算机架构，所以并不是绝对准确的。

2. EgoVLPv2的swithching能力使作者团队能够在相同的VLP框架下统一双编码器和融合编码器，去共同为基础的下游任务减少任务特定的微调成本，通过采用相同的预训练交叉注意模块跨不同的视频语言任务。

3. 作者团队在8个第一视角的基准上证明了EgoVLPv2的有效性，并在几个相同规模大小的骨干网络中实现最先进的性能。如下所示：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/0567b29f767c359f56b2069e55917696.png#pic_center)


对于表中的各项数据指标，可以访问[这里](https://blog.csdn.net/weixin_39188311/article/details/132454365?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22132454365%22%2C%22source%22%3A%22weixin_39188311%22%7D)进行详细的介绍，这一节作者团队主要聚焦于论文本身的工作，对于评估指标的疑问，访问上面地址查缺补漏即可，这里就不做赘述。具体对于上面这个雷达图的表示含义，这里做一个解释：这个图总结了EgoVLPv2与EgoVLP的性能比较。首先，为了说明目的，作者通过由EgoVLPv2实现的分数归一化每个轴，其在范围（0，1）内转动轴。接下来，将每个轴的原点保持在0.7归一化值，这合理地分离了内部框架和外部框架，以获得更好的可读性。最后，用绝对性能指标分数注释每个顶点。

### 5. 相关工作

#### 5.1 VLP框架

随着图像语言预训练（ILP）的成功，视频语言预训练（VLP）在近几年收到很大的关注。VLP框架目前可以分为三大类（参见下图）：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/91fcd9455a84f72f4f9a972cebd9067f.png#pic_center)


上述图中展示了四种，最后是作者团队提出的VLP框架结构，以往三种编码器如下——

    - (a)使用单独视频编码器和文本backbone，InfoNCE 作为共同的预训练目标(INFONCE)作为损失函数，把多模态的问题转化为二分类的问题。

    - (b)在双编码器之上使用堆叠的跨模态融合层，使用MLM，VTM等。**作为常见的预训练任务，MLM中每一个模态作为分类器进行训练，然后组合两者进行新任务的训练，VTM是弱监督跨模态学习范式，每一种模态使用预训练的网络模型进行特征提取，然后使用自监督的方法把它们组合起来进行新任务的训练，这种方法不需要大量的标注数据**

    - (c)使用单个编码器用于不同的模态，具有与（b）相似的学习目标

    - (d)在骨干网络中实现融合。
那么现在具体看一下：

- 双路编码器

首先，有一个语法错误：Many existing egocentric VLP frameworks falls into this category.这句话中的falls应该为fall，接着看～现在很多的第一视角的VLP框架都遵从这种双路编码器的方式，他们使用单独的视频和语言backbone，并在微调过程中跨模态融合学习特定于具体任务的内容。它们通常使用InfoNCE 损失或MIL-NCE 损失（基于NCE损失改进的，同样也是度量两个样本之间的差异性的损失）进行训练，并在视频文本检索中取得了比较好的成功。

- 堆叠融合编码器

这种方式是在双路编码器上面使用，使用大量的参数交叉跨模态融合。

- 共享编码器

对于另外组合视频和文本然后进行学习的组合编码器的方法，则属于这一范畴。它们独立于模态，可应用于图像、视频、文本、音频、时间序列和单视图3D数据。常见的学习目标包括掩码语言建模、掩码帧建模、掩码token建模、掩码模态建模、句式排序建模、帧排序建模和视频文本匹配。

其中掩码语言建模：Masked Language Modeling（MLM）是一种自然语言处理（NLP）任务，它的目标是训练一个模型来预测在文本中被遮盖或替换的单词。在训练过程中，输入文本中的每个单词都被随机遮盖，然后模型尝试预测这些被遮盖的单词。这个过程可以帮助模型学习到词汇之间的关系和上下文信息。在实际应用中，MLM 常用于预训练大型自然语言处理模型，如 BERT、RoBERTa 和 XLM-R。通过在大量文本数据上进行 MLM 训练，模型可以学习到丰富的语义表示，从而提高其在各种下游任务上的性能，例如情感分析、命名实体识别和问答系统等。

掩码帧建模：Masked Frame Modeling（MFM）是一种视觉语言模型，它的目标是训练一个模型来预测在视频帧中被遮盖或替换的部分。在训练过程中，输入视频帧中的每个像素都被随机遮盖，然后模型尝试预测这些被遮盖的像素。这个过程可以帮助模型学习到视频帧中不同区域之间的依赖关系和上下文信息。MFM 常用于预训练大型视觉语言处理模型，如 ViLBERT、MVLM-VAE 和 MAE。通过在大量视频数据上进行 MFM 训练，模型可以学习到丰富的语义表示，从而提高其在各种下游任务上的性能，例如目标检测、语义分割和动作识别等。

掩码token建模：Masked Token Modeling（MTM）是一种视觉语言模型，它的目标是训练一个模型来预测在图像序列中被遮盖或替换的部分。在训练过程中，输入图像序列中的每个像素都被随机遮盖，然后模型尝试预测这些被遮盖的像素。**这个过程可以帮助模型学习到图像序列中不同区域之间的依赖关系和上下文信息**。

掩码模态建模：Masked Modal Modeling（MMM）是一种视觉语言模型，同上。

句式排序建模：Sentence Ordering Modeling（SOM）是一种文本排序任务，它的目标是从顺序的角度学习文本token的关系。具体而言，选择一些句子，将它们随机分成若干个片段，并随机打乱顺序。然后，模型需要预测每个片段在原始句子中的位置。这个过程可以帮助模型学习到文本中不同部分之间的依赖关系和上下文信息 。

帧排序建模：Frame Ordering Modeling（FOM）是一种文本排序任务，它的目标是从顺序的角度学习文本token的关系。具体而言，随机选择固定百分比的帧，并预测其原始顺序。这个过程可以帮助模型学习到文本中不同部分之间的依赖关系和上下文信息 。

视频文本匹配：Video-Text Matching（VTM）是一种跨模态检索任务，它的目标是在不同的模态之间找到匹配的实例。具体而言，它将文本描述与视频帧进行匹配，以便在大量视频中查找与给定文本描述相关的视频 。这个过程可以帮助用户更快地找到他们感兴趣的内容。

在（d）中展示的就是作者使用门控机制在单模态上完成跨模态融合的框架。

#### 5.2 视频语言数据集

VLP任务的成功可以部分归因于大规模开源可获取的视频文本数据集，如[ActivityNet](https://arxiv.org/abs/1705.00754)，[WebVid-2M](https://openaccess.thecvf.com/content/ICCV2021/papers/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.pdf)和[HowTo100M](https://arxiv.org/abs/1906.03327)。这些数据集包括来自网络（如YouTube）的视频，并与相应的ASR字幕配对（ASR字幕配对是指将音频和视频中的语音内容自动转换为文字，并将其与相应的字幕进行匹配。这个过程可以帮助听障人士更好地理解他们所观看的视频），使其在VLP预训练中很受欢迎。尽管它们的大小令人印象深刻，但这些现有的视频文本预训练数据集通常具有第三人称视角视图。另一方面，第一视角的视频近些年受到社区的广泛关注。以前的第一视角数据集是小规模和特定领域的。最近发布的Ego4D是第一个大规模第一视角数据集，由来自全球9个不同国家的74个地点的931人收集的3670小时的视频组成。最近，EgoClip提供了Ego4D的过滤版本，具有不同视频片段长度的间隔，而不是单个时间戳。作者训练出的算法框架模型EgoVLPv2，就是在EgoClip版本的Ego4D上进行训练的。

这里读者如果对第一视角数据集感兴趣的话可以访问[这里](https://blog.csdn.net/weixin_39188311/article/details/132638620?spm=1001.2014.3001.5502)进行阅读和浏览。

### 6. EgoVLPv2

#### 6.1 骨干网络融合

作者使用TimeSformer和RoBERTa分别作为视频和语言的主干网络。但是这种方式是无法将跨模态的交互兼顾的，因此细粒度的多模态表征信息是缺少的。现有VLP框架通过以下方式实现跨模态融合：（i）学习共享架构或在双编码器顶部堆叠融合层，或（ii）在微调期间学习跨模态融合信息。虽然前者提供了很好的跨模态表示和零样本推理能力的多模态下游任务，但是相较于后者引入了大量的融合参数。在这项工作中，作者团队插入跨模态融合（交叉注意力）到顶部几层的单模态骨干网络中，以达到上述两种想法之间的平衡。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/401a960a19db6011e656c900036264ac.png#pic_center)

计算三个目标：$L_{EgoNCE}$、$L_{MLM}$和$L_{VTM}$。作者团队以门控的机制把跨模态融合插入到单模态骨干网络中。在预训练期间，每个前向iteration包含三个步骤：【本质上就是利用门控实现双路编码器到融合编码器的随时转换】
- （I）关闭交叉注意力模块【可以看到图中虚化了交叉注意力模块】，此时的EgoVLPv2使用的是双路编码器的结构，此时的训练目标是计算$L_{EgoNCE}$。

- (II)开启交叉注意，EgoVLPv2充当起融合编码器，并且视频掩码的描述对儿被馈送到EgoVLPv2以计算$L_{MLM}$，此时是训练一个模型来预测在文本中被遮盖或替换的单词。在训练过程中，输入文本中的每个单词都被随机遮盖，然后模型尝试预测这些被遮盖的单词。这个过程可以帮助模型学习到词汇之间的关系和上下文信息。

- (III)保持交叉注意，负样本对儿视频-叙述对被馈送到EgoVLPv2中以计算$L_{VTM}$。与使用特定融合的Transformer层相比，骨干策略中的这种融合带来了轻量级和灵活的模型设计。

现在作者团队继续看上图，并仔细的剖析一下，它显示了EgoVLPv2的架构。每一个TimeSFormer编码器层都有一个划分的时空注意力模块，包含具有残差连接的时间和空间自注意力。
用下面的图中的右侧🫱表示编码器的输入输出的信息的转换关系：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/75414d04db8dbbd47d21b23a7629af31.png#pic_center)


在第$k$个编码器层的输出$z^{(k)}$可以由下面的式子进行表示：

$\begin{aligned} \hat{x}_{v i d}^{(k)} & =x_{v i d}^{(k-1)}+\operatorname{TEMP}-\operatorname{SA}\left(x_{v i d}^{(k-1)}\right) \\ z^{(k)} & =x_{v i d}^{(k-1)}+\operatorname{SPA}-\operatorname{SA}\left(\hat{x}_{v i d}^{(k)}\right) \\ & =\operatorname{SPACE}-\operatorname{TIME}\left(x_{v i d}^{(k-1)}\right)\end{aligned}$

其中的$x_{vid}^{k-1}$是第${k-1}$层编码器的输出，而$TEMP-SA$和$SPA-SA$分别表示时间和空间自注意块。通过在时空注意力之后引入门控交叉注意力，作者团队在多模态融合插入到骨干网络中来，因此，第$k$个融合TimeSformer层的输出$x_{vid}^{k}$可以表示如下：

$\begin{aligned} & z^{(k)}=\operatorname{SPACE}-\operatorname{TIME}\left(x_{v i d}^{(k-1)}\right) \\ & x_{v i d}^{(k)}=x_{v i d}^{(k-1)}+z^{(k)}+\alpha * \operatorname{CA}\left(z^{(k)}, x_{t e x t}^{(k-1)}\right) \\ & x_{v i d}^{(k)}=x_{v i d}^{(k)}+\operatorname{FFN}\left(x_{v i d}^{(k)}\right)\end{aligned}$

上面的公式也可以通过上图的左侧进行对应的量化。其中$x_{text}^{k-1}$是第$k−1$个RoBERTa层的输出，CA、FFN分别表示交叉注意力模块和前馈神经网络，α是从0初始化的可学习门控参数。每个RoBERTa层包含多头自注意力层，随后是前馈层。与融合的TimeSFormer模块类似，作者团队将交叉注意力插入到RoBERTa骨干网络中来，公式化如下：

$\begin{aligned} \hat{x}_{\text {text }}^{(k)} & =\mathrm{SA}\left(x_{\text {text }}^{(k-1)}\right) \\ x_{\text {text }}^{(k)} & =x_{\text {text }}^{(k-1)}+\hat{x}_{\text {text }}^{(k)}+\alpha * \mathrm{CA}\left(\hat{x}_{\text {text }}^{(k)}, x_{\text {vid }}^{(k)}\right) \\ x_{\text {text }}^{(k)} & =x_{\text {text }}^{(k)}+\operatorname{FFN}\left(x_{\text {text }}^{(k)}\right)\end{aligned}$

其中SA是传统的自注意力模块。为了简单起见，作者将交叉注意插入到两个主干中相同数量的层中。值得注意的是，骨干策略中的这种融合不仅限于TimeScer和RoBEERTa;但也可以应用于任何基于Transformer的视频编码器和文本编码器中来。

具有门控交叉注意的骨干中的融合具有以下优点：（i）通过将选通标量$α$设置为0，可以容易地关闭交叉注意参数;因此，该模型表现为双编码器，这对于需要“未融合”视频和文本特征的场景是有帮助的;（ii）作者的融合方法比添加融合专用Transformer层更轻量级和计算效率，这在下文中有详细的表述信息。

#### 6.2 预训练目标
使用三个损失函数作为预训练的目标：（1）第一视角噪声项对比估计（EgoNCE），（2）掩码语言建模（MLM），以及（3）视频文本匹配（VTM）。

**EgoNCE**：[Lin等人](https://arxiv.org/abs/2206.01670)为基于双编码器的EgoVLP提出了EgoNCE。它对InfoNCE做了两个修改：（i）除了匹配的视频-文本样本之外，共享至少一个名词或一个动词的所有对被视为阳性。(ii)每批N个视频文本样本对儿用另外N个视觉上相似的视频来进行增强，这些视频被视为附加的负样本。总的来说，视频到文本的EgoNCE目标损失函数$L_{v2t}^{ego}$可以表示为下面的式子：

$\mathcal{L}_{\mathrm{v} 2 \mathrm{t}}^{\mathrm{ego}}=\frac{1}{|\widetilde{\mathcal{B}}|} \sum_{i \in \widetilde{\mathcal{B}}} \log \frac{\sum_{k \in \mathcal{P}_{\mathrm{i}}} \exp \left(\frac{\mathrm{v}_{\mathrm{i}}^T \mathbf{t}_k}{\tau}\right)}{\sum_{j \in \mathcal{B}}\left(\exp \left(\frac{\mathbf{v}_{\mathrm{i}}^T \mathbf{t}_{\mathrm{j}}}{\tau}\right)+\exp \left(\frac{\mathbf{v}_{\mathrm{i}}^T \mathbf{t}_{\mathbf{j}^{\prime}}}{\tau}\right)\right)}$

其中，第$i$个视频嵌入信息$v_i$和第$j$个文本嵌入$t_j$是$L_2$归一化特征，并且$τ$是温度因子。$\widetilde{\mathcal{B}}$是一个2N样本的扩增批次，$\sum_{k \in \mathcal{P}_{\mathrm{i}}} \exp (\frac{\mathrm{v}_{\mathrm{i}}^T \mathbf{t}_k}{\tau})$是调整的正样本，$\exp \left(\frac{\mathbf{v}_{\mathrm{i}}^T \mathbf{t}_{\mathbf{j}^{\prime}}}{\tau}\right)$是调整的负样本，而文本到视频的 EgoNCE目标是$L_{t2v}^{ego}$，它和视频到文本的EgoNCE是一致类似的定义：

总的EgoNCE如下定义：
$\mathcal{L}_{EgoNCE}=\mathcal{L}_{t2v}^{Ego}+\mathcal{L}_{v2t}^{Ego}$, 作者团队在双编码器设置中计算EgoNCE。具体来说，设置了α = 0，因此，关闭交叉注意模块来计算EgoNCE。

**MLM**：掩码语言建模（Masked Language Modeling，MLM）和视频-文本匹配（Video-text matching，VTM）在基于融合编码器的VLP框架中被证明是有用的。对于MLM，具体操作就是随机屏蔽15%的文本tokens【和BERT有类似，把这15%分解为10%的随机单词，10%不变，80%带有特殊标记\[MASK\]】，损失函数$L_{MLM}$，旨在通过最小化负对数似然来基于前后周围的单词和视频patches重建被掩码遮住的token。

**VTM**：对于VTM目标，模型输入的是视频文本样本对儿，并且输出是指示输入对是否匹配的二值标签$y ∈ {0，1}$。$L_{VTM}$被构造为在预测分数上的二值交叉熵损失函数。作者使用EgoNCE计算的相似性来对全局hard-negative视频-文本对进行采样。作者在融合编码器设置中计算$L_{MLM}$和$L_{VTM}$。在这种情况下，α 不可以是 0也就意味着交叉注意模块被打开。总的来说，EgoVLPv2预训练pipeline可以总结为以下三个步骤：

- **EgoNCE**：需要没有融合的视频和文本特征信息，因此需要关闭交叉注意力模块（通过α = 0进行门控开合操作）。因此，利用充当双路编码器的EgoVLPv2来计算$L_{EgoNCE}$。
- **MLM&VTM**：这里需要多模态的表征信息，此时的EgoVLPv2中使用到交叉注意力模块并充当融合编码器完成跨模态信息的整合。
- 对于反向传播，将三个损失函数进行相加，得到总的损失函数为：
$L_{total} =（1−γ−δ）L_{EgoNCE}+γL_{MLM}+δL_{VTM}$，
并反向传播到端到端模型中。γ和δ是控制不同项对$L_{total}$的贡献的超参数。关于EgoVLPv2的不同训练前目标的消融见下文。用于预训练EgoVLPv2的伪码可以在补充中找到。
 
#### 6.3 适应下游任务

这个部分是描述如何使EgoVLPv2适应不同的下游任务，如下图所示。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ec4245e3a84dcd19be5f2c10f8898859.png#pic_center)
 

EgoVLPv2可以适应各种基于双重和融合编码器的视频语言任务，从检索，视频问答和视频接地到以查询为中心的视频摘要。

**视频文本检索**：作者在两种设置中执行检索：（I）双路编码器：关闭交叉注意力模块，并使用EgoVLPv2作为双路编码器，去计算视频片段和文本注释之间的余弦相似性。(II)融合编码器：切换到交叉注意力模块开启的状态。视频和语言主干的顶部M层交互并产生多模态表示，其被馈送到预训练的VTM（视频文本匹配）头部以计算匹配分数。作者还计算了这两种方法的集合，以进一步提高性能。

**视频定位和问题解答**：作者执行单一（仅视频）和多模态（文本引导）视频定位。作者关闭了单峰接地的交叉注意，只使用视频编码器。作者使用EgoVLPv2作为融合编码器的文本引导接地和视频问答。

**以查询为中心的视频摘要**：对于此任务，输入视频非常长（3-5小时）。作者首先使用视频和文本编码器的未融合$N-M$层，从5秒片段和文本查询中提取单模态特征。接下来，应用[KTS镜头边界检测器](https://pdfs.semanticscholar.org/7d04/a4c9e7fb0e5aa9760951029cb295d70ff6e9.pdf)来分割长视频。在此之后，查询和分段片段特征被馈送到EgoVLPv2的顶部$M$个融合层中以计算多模态表示。最后，学习了一个额外的单层Transformer来设计每个片段中所有5秒长片段的相互关系。作者提出了额外的细节查询为重点的视频摘要框架的补充。

具体说来，对于上面的预训练EgoVLPv2算法，有如下详细表示：
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9d124dcf99253314cb3b612b112fd476.png#pic_center)


### 7. 实验

#### 7.1 预训练和下游任务
作者在Ego4D的EgoClip版本上预训练了EgoVLPv2，这是最大的公开可用的以自我为中心的视频数据集。EgoClip来源于没有修剪的第一视角数据集，它提供过滤的视频描述样本对，具有可变长度的片段间隔，而不是Ego4D的单个时间戳。此外，EgoClip排除了出现在Ego4D基准测试的验证集和测试集中的视频，导致大约380万个预训练样本覆盖了来自129个不同场景的超过2927小时的视频。
作者在五个第一视角数据集上跨多个基准评估EgoVLPv2，总结在下面：

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e675e04229fb4e465374e8dcc5153baa.png#pic_center)


第一视角相关的下游任务数据集、指标和评估。作者团队在各种各样的基准上评估EgoVLPv2算法模型，在视频文本检索任务中（包括EgoMCQ，CharadesEgo，EpicKitchens-100等），单模态和文本引导的视频定位（EgoMQ，EgoNLQ），视频问答任务（EgoTaskQA）和以查询为中心的视频摘要（QFVS）。评估方式包括零样本（Zero-shot），特定于任务的头部微调任务（HT），和端到端的微调任务（Fine-Tuning）。注意的是ChardesEgo本身是一个多类分类的问题，但作者将这个任务转换为检索任务。

作者在五个基准上进行评估，作者现在分别对其用到的五个基准进行展开描述：在Ego4D的基准测试中：多项选择题（Ego Multiple Choice Questioning, EgoMCQ）是一个文本到视频（T → V）的检索任务，每个查询文本有五个视频片段。自然语言查询任务（EgoNLQ）是一个自然语言基础任务，旨在定位给定了文本查询的视频内的单个时间间隔。Moment Query（EgoMQ）是一个仅视频的时间动作定位任务。这么如果不好理解的话，作者下面详细展开进行一个描述：

- 【EgoMCQ】：EgoMCQ（Ego-generated Multiple-choice Questions）是一个第一视角（first-person）的文本视频检索任务，其主要目标是基于视频中的人的视角产生一系列的查询问题，然后从视频片段中寻找和作者产生的问题的答案最相符或者近似相符的片段。这个任务对于理解视频内容、提升视频检索效果、提高自然语言和视觉信息的交叉融合都具有重要的意义。
    
    如下是作者针对这个任务的一个具体的描述：

    【流程概述】：

    给定一个包含人物动作、事件和物体交互的短视频片段。算法模型会从视频中提取与人物视角相关的信息，生成一系列具有挑战性的、可能涵盖视频大部分内容的问题。然后从原始视频或已标记的视频数据集中寻找与问题相对应的答案的片段。例如，一个EgoMCQ任务可能会是这样的：在一段打篮球的视频中，问题可以是“谁投了那个球？”或者“那个球是如何被投进去的？”然后算法需要从视频中找到与问题相对应的片段。
    
    【难点】：
    
    视角理解：为了生成能够全面涵盖视频内容的问题，需要深入理解人物的自身视角和他们在视频中关注的焦点是什么。
    
    上下文理解：为了理解视频中发生的事件和交互，需要上下文的信息。例如，在篮球视频中，可能需要了解谁在控制球以及他们在比赛中的角色。
    
    动作和事件的精确描述：视频中的事件和动作通常涉及到复杂的空间和时间关系，对这些关系进行精确描述构成了理解的另一个难点。

    视频片段的检索：在给定的问题和答案片段之间建立有效的对应关系，也是一个重要的问题。这通常涉及到复杂的视频特征提取、匹配和排序算法。
    
    自然语言和视觉信息的交叉：将自然语言问题转化为视觉问题，然后再从视觉信息中找到对应的答案，这个过程中需要解决很多技术挑战。例如，如何把自然语言的问题转化为视觉可理解的问题（比如上述例子中的“谁投了那个球？”可以被转化为“找出视频中投篮的人”），以及如何从视觉信息中找到对应的答案等。

    【具体的流程】：可以分为以下几个步骤：

    1. 视频预处理：包括视频帧的提取、人物和物体的检测与识别、背景信息的获取等。

    2. 问题生成：基于人物的视角和视频中的事件、动作等信息，生成一系列可能的问题。
    
    3. 答案检索：通过检索算法（如基于深度学习的特征提取和匹配算法）在视频中找到与问题相对应的答案片段。
    
    4. 结果评估：通常采用准确率、召回率和F1分数等指标来评估答案检索的效果。
    
    5. 反馈循环：根据评估结果，对问题生成和答案检索的算法进行优化和改进。
    ---

- 【EgoNLQ】：指的是基于第一人称视角利用自然语言进行查询。主要任务是通过理解用户输入的自然语言查询语句，从大量的数据或信息中快速、准确地找出与用户查询意图相关的结果。具体来说，NLQ需要处理用户输入的自然语言文本，提取其中的关键词、短语或句子，分析其语义和意图，然后根据这些信息在相关数据源中查找匹配的结果，最后将结果返回给用户。

    NLQ的具体实现流程一般包括以下步骤：

    1. 文本预处理：对用户输入的自然语言文本进行预处理，包括分词、词性标注、命名实体识别等操作，以便进一步分析其语义和意图。
    
    2. 语义分析：利用NLP技术对预处理后的文本进行语义分析，包括句法结构分析、实体关系识别、情感分析等操作，以确定用户的查询意图。

    3. 信息检索：根据语义分析的结果，在相关数据源中查找与用户查询意图匹配的信息或数据，一般需要进行相关性排序和去重操作。

    4. 结果呈现：将检索到的结果按照相关性和排序进行呈现，以便用户选择和获取所需的信息。

    5. 反馈与优化：根据用户的反馈和使用情况，不断优化NLQ模型和算法，提高其准确性和效率。
    ---

- 【EgoMQ】：Moment Query是一种自然语言查询任务，旨在从文本中检索与特定时间点或时间段相关的信息。它涉及从自然语言描述中提取时间信息，并根据该时间信息从给定的文本中找到相关的内容。

    具体来说，Moment Query任务的目标是，给定一段文本和一个时间戳或时间范围描述，从文本中找到与该时间戳或时间范围相关的内容。这个任务在很多场景下都很有用，例如，用户希望查找某个特定时间点发生的新闻事件，或者想了解某位人物在某个时间段内的经历等。

    实现Moment Query任务的主要难点在于以下几个方面：

    时间信息抽取：需要从自然语言描述中提取出准确的时间信息，包括时间戳和时间范围。这需要对自然语言进行深入的理解和分析，包括词法分析、句法分析、实体识别等。
    
    文本检索：在给定的文本中找到与时间信息相关的内容，需要考虑文本中的时序关系、事件触发词、上下文信息等多种因素。这需要对文本进行深入的语义分析和信息抽取。
    
    时间信息验证：需要验证检索到的内容与时间信息的一致性，以确保结果的准确性和可靠性。这需要对文本和时间信息进行深入的语义匹配和验证。
    
    Moment Query任务的具体实现流程一般包括以下步骤：

    1. 文本预处理：对给定的文本进行预处理，包括分词、词性标注、命名实体识别等操作，以便进一步分析其语义和结构。

    2. 时间信息抽取：利用自然语言处理技术对用户输入的自然语言文本进行时间信息抽取，包括时间戳和时间范围等，以便进一步匹配和筛选。

    3. 文本检索：根据时间信息在给定的文本中查找与之相关的事件或实体，并进行初步筛选和排序，以便进一步验证和筛选。

    4. 时间信息验证：对初步检索到的内容进行时间信息验证，以确保结果与给定的时间信息一致，并进一步提高结果的准确性。
    
    5. 结果呈现：将验证后的结果按照相关性和排序进行呈现，以便用户选择和获取所需的信息。
    ---

- 【以查询为中心的视频摘要（QFVS）】：旨在基于自然语言查询生成长（3-5小时）第一视角视频的简洁缩略版本。Query-focused video summarization是一种针对视频数据的自然语言查询任务，旨在将用户提供的查询意图转化为可执行的查询语句，从而从大量视频数据中找到与用户查询相关的关键信息。

    具体任务的流程包括以下步骤：

    1. 用户输入查询语句，描述对视频内容的关注点或查询意图。例如，用户可能希望查询关于“篮球比赛中球员扣篮的片段”的视频片段。

    2. 对用户输入的查询语句进行自然语言处理，包括分词、词性标注、命名实体识别等操作，以确定用户关注的实体、动作等关键信息。

    3. 将处理后的查询语句转化为可执行的查询语句，以便从视频数据中检索相关内容。例如，对于上述查询意图，可以转化为类似于“找到所有篮球比赛中球员扣篮的片段”的查询语句。
    
    4. 对所有视频数据进行处理和检索，将符合查询条件的关键帧或镜头提取出来，并对这些片段进行排序和整合，生成相应的视频摘要。
    
    5. 将生成的摘要呈现给用户，用户可以通过观看摘要来评估摘要的质量，并提出反馈意见。
    
    【研究难点】：

    1. 用户查询的多样性和主观性：不同用户对同一视频内容可能存在不同关注点或查询意图，如何准确理解并处理用户输入的查询语句是一个难题。
    
    2. 视频数据的复杂性和大规模性：现实场景中的视频数据往往具有复杂性和大规模性，如何高效地处理和检索这些数据也是一个挑战。
    
    3. 摘要生成的质量评估：如何评估生成的摘要质量是一个重要问题，因为不同用户可能对摘要的质量有不同的评价标准。

- 【EgoTaskQA】：EgoTaskQA上的视频问答任务提供了四种问题类型（描述性、预测性、解释性和反事实性），具有直接和间接的参考，并评估了目标导向任务理解的空间、时间和因果域的预测。值得注意的是，根据作者的表述，他们是第一个将QFVS和EgoTaskQA统一为VLP框架的两个下游任务的工作。

    Video question-answering，或VQA，是一种涉及计算机视觉和自然语言处理的任务，它让计算机能理解和分析视频内容，并能用自然语言回答关于这些视频的特定问题。
    
    具体来说，VQA系统的流程和详细步骤包括以下几点：

    1. 输入：给定一个视频和一个关于这个视频的自然语言问题。
    2. 处理：首先，VQA系统需要理解和分析视频的内容，识别和提取视频中的关键信息，如人物、物体、动作、场景等。同时，它也需要理解问题的含义和意图，这可能涉及到一些关键词或短语的理解。
    3. 搜索：然后，VQA系统会在其知识库中搜索与问题相关的信息或答案。这可能涉及到对整个视频内容的深度搜索，也可能只针对视频中的某些特定片段或细节。
    4. 生成答案：最后，VQA系统会根据搜索结果生成一个自然语言答案。这个答案需要符合问题的语境和知识库中信息的特点，让人能够理解并相信。

CharadesEgo：对日常室内活动进行多类分类，类名是简短的自然语言短语，如“把东西放在架子上”。因此，利用类名称的文本表示，作者把这个任务作为一个检索问题。

Epic-Kitchens-100（EK 100 MIR）上的多实例检索：这是一个文本到视频（T → V）和视频到文本（V → T）的检索任务，不同的叙述之间具有显著的语义重叠。

关于下游任务的设定
**EgoNLQ**：这个任务是一个视频-文本定位的问题，每个视频片段最长可达1200秒。因此，在EgoNLQ上执行端到端的微调可能很困难。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/d02b26981dcd96c19c77860b8bc6a4f4.png#pic_center)


上图展示的是作者使用预训练的EgoVLPv2预提取视频文本特征，并在冻结编码器上训练VSLNet。作者使用预训练模型从视频文本样本中预提取特征，并训练VSLNet了100个epoch，学习率为1 e-3，批次大小为32。保持所有其他配置与baseline相同。然而可以看到可以使用更小的任务头和更少的微调时间来击败baseline。

**EgoMQ**：这是一个仅视频的定位问题，与EgoNLQ类似，输入视频非常长。因此，端到端的微调也很难在EgoMQ上执行。在EgoVLP之后，使用预训练的EgoVLPv2预提取视频特征，并训练VSGN 100个epochs，学习率为1e-4，批量大小为32。保持所有其他配置与VSGN相同。执行网格搜索VSGN的其他超参数。

**QFVS**：以查询为中心的视频摘要旨在生成由自然语言查询引导的输入视频的删节版本。此任务的输入视频非常长（3-5小时）。作者首先使用视频和文本编码器的未融合N-M层从每5秒的片段和文本查询中提取单模态的特征信息。接下来，应用KTS边界检测器来分割长视频。之后，查询和分段片段特征被馈送到EgoVLPv2的顶部M个融合层中以计算多模态表示。最后，学习了一个额外的单层Transformer来设计每个片段中所有5秒长片段的相互关系。使用AdamW优化器、余弦调度器和线性预热器训练单层Transformer达到20个epoch，批量大小设置为20，峰值学习率为1e −5。

**EgoTaskQA**：作者将视频QA视为一个分类问题，其中在预训练的EgoVLPv2生成的融合特征表示之上训练线性层。在微调设置中，使用AdamW优化器对预训练模型进行了36个epoch的微调，批量大小为64。使用10%线性预热步骤的余弦退火策略，direct splits的峰值学习率为2e−4，indirect splits的峰值学习率为1e−4。在head-tuning设置中，仅在具有相同配置的冻结主干之上训练分类器头部。

>EgoTaskQA" 中的 "direct" 和 "indirect" splits 是指在任务化问答数据集中的两种不同数据划分方式，这些数据划分方式通常用于评估模型在不同情境下的性能。以下是它们的具体解释："Direct" Split（直接划分）："Direct" split 是一种数据划分方式，其中问题（queries）和答案（answers）直接从原始上下文（context）中提取。在 "Direct" split 中，问题和答案通常都是直接从文本中截取出来的，没有进行过大的修改或变换。这个划分方式旨在测试模型在处理直接从文本中提取的问题和答案时的性能，模型需要根据上下文信息来理解和回答问题。"Indirect" Split（间接划分）："Indirect" split 是一种数据划分方式，其中问题和答案可能会经过一些间接或转换步骤，而不是直接从原始上下文中提取。在 "Indirect" split 中，问题和答案可能经过重写、重组、抽象化等处理，以增加问题的多样性或复杂性，或者以测试模型的推理能力。这个划分方式旨在测试模型在处理更具挑战性或抽象性的问题和答案时的性能，模型需要不仅理解文本内容，还需要进行更多的推理和理解。总的来说，"Direct" split 和 "Indirect" split 是为了评估模型在不同类型的任务化问答情境下的性能而设计的两种不同数据划分方式。"Direct" split 更侧重于基本的文本理解和问题回答能力，而 "Indirect" split 更侧重于模型的推理、抽象化和复杂问题处理能力。研究人员使用这些划分方式来全面评估模型的能力，以便更好地了解其在不同任务和应用中的潜力和限制。

**CharadesEgo**：作者将CharadesEgo转换为检索问题。在零样本实验设置中，执行基于双路编码器的推理。在fine-tuning设置中，作者使用EgoNCE作为目标函数。使用AdamW优化器对模型进行了10个epoch的微调，批次大小为128，（β1，β2）=（0.9，0.98），权重衰减为0.01。我们使用带预热的余弦退火，线性预热步长为10%，峰值学习速率为1.5e−4，结束学习速率为1e−7。由于这是一个多类数据集，其中每个视频可以包括多个动作，所以作者用mAP作为评估指标。对于输入的信息，是从每个视频片段中采样16帧，并将这些帧reshape为224 × 224的分辨率。

**EK-100 MIR**：由于叙述可以与EK-100多实例检索任务的多个视频联合关联，因此作者使用自适应多实例最大间隔损失函数用于该任务，边缘值为0.2。
>自适应多实例最大间隔损失（Adaptive Multi-Instance Max-Margin Loss）是一种损失函数，通常用于监督学习任务，特别是在多实例学习（Multi-Instance Learning，MIL）中。这种损失函数旨在训练模型以进行实例级别的分类，其中训练数据以包（bag）的形式呈现，而不是单个实例。以下是对 Adaptive Multi-Instance Max-Margin Loss 的一些关键概念和说明：多实例学习（MIL）：在多实例学习中，训练样本被组织成包（bag），每个包中包含多个实例。这些实例可以是正类别（positive）或负类别（negative）。任务的目标是根据包中的实例来预测包的标签（是正类还是负类）。最大间隔损失（Max-Margin Loss）：最大间隔损失是一种常用的损失函数，它旨在确保模型将正类别和负类别的实例分开得尽可能远，以提高分类性能。通常，这种损失鼓励正类别实例的分数（得分）高于负类别实例的分数。自适应（Adaptive）："自适应" 表示损失函数可以自动适应不同包（bag）的特性和实例的权重。这意味着对于不同的包，损失函数可以根据实例的重要性进行调整，以更好地训练模型。在 Adaptive Multi-Instance Max-Margin Loss 中，损失函数会根据包的组成和实例的重要性来自动调整。这有助于处理不均衡的包，其中一些包可能包含大量正类别实例，而其他包只包含少数正类别实例。通过自适应地调整损失函数，可以更好地处理这些情况，确保模型对于不同包的分类具有良好的性能。

作者保持零样本配置与CharadesEgo相同。使用AdamW优化器（β1，β2）=（0.9，0.98）和权重衰减0.01，对模型进行了100个epoch的微调，批量大小为128。使用带预热的余弦退火，线性预热步长为10%，峰值学习速率为2 e −4，结束学习速率为1 e −7。

#### 7.2 评估

作者使用三种评估方案评估EgoVLPv2：

*第一种方案：*
- 零样本（Zero-Shot）。对于这个任务，预训练的骨干网络会直接应用于视觉和文本之间的检索，而无需对下游数据集进行微调。作者团队通过以下方式执行零样本检索任务：
    - （i）双路编码器结构，计算视频片段和文本叙述之间的余弦相似性。
    - （ii）融合编码器，结合预训练的VTM头以计算视频-文本匹配分数

*第二种方案：*
- 任务特定的Head-tuning（HT）。作者使用冻结编码器提取特征，之后使用特定于下游任务数据集中的训练集来训练特定于任务的三个头——分别是VSLNet用于EgoNLQ，VSGN用于EgoMQ，单层Transformer编码器用于视频文本摘要，线性层用于视频QA。
具体的VSLNet用于EgoNLQ。

其中下面的图分别表示的是VSLNet和VSGN：
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/22f2aa96021e7f40ac386fa17feec842.png#pic_center)
![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/069cfc33764dc5d71bb1ad53d7af6003.png#pic_center)



*第三种方案：*
- 微调（FT）。使用下游数据集中的训练集对整个预训练的视频文本模型进行端到端微调。


#### 7.3实现细节
作者使用TimeSformer-B和RoBERTa-B作为视频和语言的主干网络。该视频编码器有12层12个头，并配置有16 × 16的patch size大小和768的hidden dimension。空间注意力模块从ViT初始化而来。作者将视频大小调整为224 × 224，每个视频采样4帧用于预训练，16帧用于下游任务的微调。作者使用预训练的RoBERTa-B。对于模型的最佳选择策略，作者融合了两个编码器的顶部6层。作者使用AdamW 对模型进行了20个epoch的预训练，batch size大小为256，骨干网络的峰值学习率为3e-5，交叉模态参数的峰值学习率为12e-5。作者在前2个epoch使用线性预热linear warmup并使用线性衰减linear decay。预训练需要5天，使用了32个A100 GPU。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ebc01fb40cebff3369ea1e2b73d18ad2.png#pic_center)


#### 7.4 主要的结果

作者使用黑体和下划线分别表示每个表中性能最好和次好的方法，并使用Δ表示性能优于最先进的方法。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/03d8a66e742551b5c97e02a46ba2c451.png#pic_center)

上面这张表1展示的是不同的算法在EgoMCQ和EgoNLQ验证集的性能表现。EgoVLPv2在这两项任务上都比现有基线产生了显著的收益。灰色的LAVILA在GPT-2生成的15倍以上的叙述上进行预训练。在EgoMCQ上，其结果是通过直接集成双重和融合编码器为基础的推理来实现的。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/e93002ee8a8ea80e8aa0aff29adb54da.png#pic_center)

上面的表2展示的是EgoMQ的验证集上的性能。EgoVLPv2使用VSGN作为定位头。

**Ego4D**：上面的两张表呈现了EgoVLPv2在三个不同的Ego 4D基准上的性能：EgoMCQ、EgoNLQ和EgoMQ。在EgoMCQ上，模型实现了91.0%的视频间和60.9%的视频内准确度，比基线有了显著的提高。
>这里的视频间和视频内的区别如下：视频间Inter-video包括了从不同视频中随机选取的五个片段来形成问题选项。这种分组策略是为了增加问题的挑战性，因为随机选取的片段往往来自不同的视频，因此在内容上可能会有很大的差异。这种差异可以帮助区分不同场景下的实例。与此相反，视频内intra-video则是从同一视频中选取五个片段来形成问题选项。这种分组策略可以使问题更关注视频内部的细节和变化，因为所有的片段都来自同一个视频，所以在内容上可能会有更多的相似性和连续性。这对于那些需要关注视频内部发展和变化的问题特别有用。

请注意，EgoVLPv2在具有挑战性的视频内intra-videoMCQ任务上比LAVILA实现了1%的绝对增益，LAVILA使用由预训练的大型语言模型GPT-2生成的15倍以上的叙述进行训练。在EgoNLQ上，对于IoU = 0.3，EgoVLPv2相对于EgoVLP产生2.11%R@1的令人印象深刻的增益。同时，使用更小的特定于具体任务的head和更少epoch的head-tuning，EgoVLPv2优于现有的基线，这表明在预训练期间学习跨模态信息的重要性。

在单模态定位任务EgoMQ上，EgoVLPv2框架也达到了一个新的SOTA。

>这里的IOU和R分别有其具体含义

>IOU测量两个文本区域（通常是一段文本或一段连续的文本）之间的重叠程度。计算方法如下：$\mathcal{IOU = (交集区域的字符数) / (并集区域的字符数)}$.在NLQ任务中，IOU常常用于评估机器生成的答案与人工标注的参考答案之间的匹配程度。这个和CV视觉中的有类似的概念含义。通常，如果IOU值接近1，表示生成的答案与参考答案高度重叠，匹配程度很高。如果IOU值接近0，表示生成的答案与参考答案几乎没有重叠，匹配程度很低;

>R在NLQ任务中表示系统为了响应用户的查询而对检索结果进行的排序或排名，以提供最相关的信息。对于具体的含义R@1，R@5或者R@10分别具有的含义如下：R@1 是一种评估指标，通常用于衡量信息检索或排名任务中的性能。具体来说，R@1 表示在排名列表的顶部（即排名第一位）是否存在与用户查询或问题最相关的答案或结果。R@1 常常用于评估系统的单一最佳答案性能。如果 R@1 为1，表示系统在排名列表的顶部成功找到了最相关的答案，这被认为是很好的性能。如果 R@1 为0，表示系统未能在排名列表的顶部找到最相关的答案，这被认为是性能不佳。R@1 是信息检索和排名任务中常见的评估指标之一，特别适用于问答系统、搜索引擎、推荐系统等需要从一个候选池中选择一个最相关答案的任务。R@5 表示在排名列表的前5位中是否存在与用户查询或问题最相关的答案或结果。在这里，"@"符号表示"at"，用于指示在排名列表的特定位置（这里是第5位）上是否存在相关答案。R@5 比 R@1 更具宽容性，因为它允许系统在前5个位置中找到相关答案，而不仅仅是第一个位置。R@1 下面的数据表示在排名列表的前一个位置（即排名第一位）是否存在与用户查询或问题最相关的答案，并且系统成功找到最相关答案的概率为 67.5%。这个数字是一个性能指标，用来衡量系统在给定查询或问题的情况下，在排名列表的第一个位置上提供相关答案的准确度。此外，还可以使用其他排名指标，如R@k（在前k个排名中是否包含最相关答案）、Mean Average Precision（平均精度）等，以更全面地评估系统的性能。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/0d2b7a3abb197900000dda7d735bd76b.png#pic_center)

上面这个表3展示的是不同的算法在以查询为中心的视频摘要（QFVS）任务中的性能。现有的基线是端到端训练的，而EgoVLPv2只在预训练的编码器上学习一个head。

**QFVS**：不同的算法在以查询为中心的视频摘要任务（Query-Focused Video Summarization，QFVS）上评估EgoVLPv2。QFVS数据集仅包含135个视频查询训练样本，其中包含长（3-5小时）视频，并且所有现有基线都是端到端训练的。相比之下，作者在预训练的编码器上学习一个微小的头（单层Transformer）。如表4所示，作者的算法在该数据集中的所有四个视频中始终达到最先进的F-1score。预训练的视频语言表示帮助EgoVLPv2实现强大的性能，而由于训练集较小，基线难以学习良好的跨模态特征。
>F1分数（F1-score）是一个用于衡量二分类模型性能的常见指标，它结合了模型的精确度（Precision）和召回率（Recall）两个指标，提供了一个综合评估模型性能的度量方式。下面是F1分数的定义和公式：精确度（Precision）：精确度是指模型正确预测为正类别的样本数与所有预测为正类别的样本数的比例。它表示模型的预测有多少是真正的正例。精确度的公式如下：精确度 = (真正例) / (真正例 + 假正例)召回率（Recall）：召回率是指模型正确预测为正类别的样本数与所有真正的正类别样本数的比例。它表示模型有多大程度地捕捉到了正例。召回率的公式如下：召回率 = (真正例) / (真正例 + 假负例)。即F1分数：F1分数是精确度和召回率的调和平均数。它将这两个指标结合在一起，旨在平衡模型的准确性和覆盖率。F1分数的公式如下：F1分数 = 2 * (精确度 * 召回率) / (精确度 + 召回率)。F1分数的取值范围在0到1之间，其中1表示完美的模型性能，0表示最差的性能。F1分数越高，表示模型在精确性和召回率之间取得了更好的平衡。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/2b04e4ec461a28b4a6c1682d1921f084.png#pic_center)

上面的表中展示的是不同的算法在EgoTaskQA任务中直接direct和间接indirect拆分的性能。EgoVLPv2在所有设置、指标和数据拆分方面都优于先前的工作。

**EgoTaskQA**：上面的表中展示的是关于EgoTaskQA数据集上的以自我为中心的视频问答任务的结果。作者的模型在微调中的各种基线上取得了显着的收益。值得注意的是，EgoVLPv2在具有挑战性的indirect splits中表现良好，这证明了其解决复杂参考任务的能力。在head-tuning中，作者只在冻结的编码器上学习到一个线性层，其中EgoVLPv2以很大的优势击败EgoVLP，这证明了交叉模态预训练表征信息的有效性。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/307e51ce80985b26bf83847e471f655a.png#pic_center)

上面的表中展示的是不同算法在CharadesEgo和EK-100 MIR两个基准上的性能表现水平。EgoVLPv2在这两项任务的微调和零样本设置方面都取得了显著的进步。结果实现了基于双编码器的推理。

**CharadesEgo**：这是一个多类别的动作识别任务，类名为短文本短语。作者将其转换为CLIP中的视频到文本（V→ T）检索问题，并执行基于双编码器的检索。如上表中所示，EgoVLPv2在微调和零样本两者中获得SOTA。

**EK-100**：上表也示出了作者对EK-100MIR的结果。在微调中，EgoVLPv2实现了相较于监督方法（S3D、MME、JPoSE）和VLP方法（EgoVLP、HierVL）的显著提升优势。在零样本的设置中，EgoVLPv2以7.8%mAP和4.4%nDCG分数击败EgoVLP和HierVL。一致的性能增益再次显示了预训练编码器的质量。

#### 7.5 消融实验

**骨干网络中的融合**：将融合模块与使用特定于融合的Transformer层的传统实践进行比较，按照ALBEF实现，下表展示出了作者所提出的融合策略比堆叠的融合层表现稍好。对于这两种方法，将融合层的数量增加到6会带来非平凡的性能增益。然而，作者提出的架构是更显著的参数效益和计算效率。例如，在6个融合层的这一项，作者所提出的架构包含33M甚至更少的参数，并需要45%的计算成本，这表明作者的方法的有效性。 

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/9500aae9f9e5368432fd4b3081b8a1d1.png#pic_center)

上面的表展示的融合策略的消融研究。作者提出的融合在骨干策略中的性能略好于使用特定于融合的Transformer层，而且具有更少的参数和更少的计算。
> 表中的"GMACs per instance" 是一个评价算法模型性能相关的指标，它衡量了在进行图像处理或计算机视觉任务时，模型的计算复杂度。GMACs 是指“亿次乘法累加操作”（Giga Multiply-Accumulate Operations）的缩写，它是一个衡量模型计算量的单位，通常用于测量卷积神经网络（CNN）等深度学习模型的计算复杂度。具体来说，"GMACs per instance" 表示每个输入实例（通常是一张图像）在模型中进行的GMACs数量。这个指标有助于了解模型在单个输入上的计算要求，因此它可以用来评估模型的计算效率和速度。通常情况下，更低的 "GMACs per instance" 表示模型更高效，因为它需要更少的计算资源来处理每个输入，而更高的值则表示模型在每个输入上需要更多的计算资源。这个指标对于选择适合特定应用场景的模型或优化模型的计算性能非常有用，特别是在资源有限的设备上部署深度学习模型时，如移动设备、边缘设备或嵌入式系统。通过降低 "GMACs per instance"，可以提高模型的计算效率，从而更好地适应这些资源受限的环境。

**预训练目标损失函数的设计**：作者对不同的预训练目标损失进行消融实验，并使用EgoVLPv2在EgoMCQ上进行评估，使用EgoVLPv2作为双编码器、融合编码器，通过为每个视频-文本对求和它们的相似度分数来对这两种方法进行整合。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/44bfaf8bd04918011c91ec1f9b6ea9ae.png#pic_center)


如上表所示，移除任何预训练目标损失都会导致性能上的下降。具体来说，VTM与hard-negative mining在很大程度上是有益的，在所有三个评估策略中。基于融合编码器的比双路编码器的有显著的改进，此外，因为EgoMCQ任务中的每个视频只包含5个序列，两种评估方法提供了类似的延迟。将两者结合在一起，可以在视频间和视频内准确度指标上进一步获得1 - 2%的性能增益。

#### 7.6 注意力可视化和误差分析

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/da625b4b529c2e9f901a33173679f078.png#pic_center)

在上图中，不同的head在跨模态的注意力层可以参与由描述注释引导的不同的语义区域的视频帧。可以观察到，预先训练的模型能够很好地识别第一视角动作中出现的各种各样的对象目标，例如室内家具、烹饪用具、电话、平板电脑、汽车转向、自行车把手等。在预训练期间学习的这种强跨模态信息有助于EgoVLPv2进行多模态下游任务。上图中的可视化是使用960p视频帧获得的，16 × 16patches的3601个token的序列。然而，在杂乱的环境中，特别是在低光条件下，被极大地阻碍了目标对象偶尔不聚焦的现象产生。

### 8. 结论

这项工作提出了了EgoVLPv2，第二代第一视角视频语言预训练模型，并通过将跨模态融合直接纳入视频和语言主干，对上一代进行了显著地改进。作者提出的融合骨干战略是轻量级且计算效率高的，这允许作者统一各种VL任务在一个灵活和有效的方式。作者进行了大量的实验，证明了EgoVLPv2在广泛的下游任务上的有效性，始终实现的是最先进的性能。此外，作者直观地展示了交叉注意力表征信息的有效性。

> 以上的内容出自原始论文，未经允许请勿转载，欢迎大家讨论和学习。

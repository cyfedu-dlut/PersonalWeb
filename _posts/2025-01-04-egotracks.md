---
title: '【每周一个数据集】EgoTracks'
date: 2025-01-04
permalink: /posts/2025/01/egotracks/
tags:
  - egocentric dataset
  - ego4d
  - visual object tracking
  - visual localization
---

第一人称视角长期跟踪数据集，EgoTracks，源于Ego4D数据集，并提出一个基线算法：EgoSTARK。

# EgoTracks：一个长期的以自我为中心的目标跟踪数据集
## 摘要
视觉目标跟踪是许多自我中心视觉问题的关键。然而，具身智能所面临的以自我为中心的跟踪挑战在许多现有的数据集中都没有得到充分的体现，这些数据集往往集中在简短的第三人称视频上。以自我为中心的视频有几个区别于过去数据集中常见的特征：频繁的大型摄像机运动和手部与物体的交互通常会导致遮挡或目标物体离开视野退出到视频帧外，并且由于视角、尺度或物体状态的广泛不同，物体外观可能会迅速变化。跟踪本身这个问题也应该是长期的，并且能够在生命周期内一致地（重新）将对象与它们的出现和消失相关联是至关重要的。以前的数据集强调这种重新检测问题，它们的“框架”性质导致了各种时空先验的采用，作者团队发现这些先验不一定适用于以自我为中心的视频。因此，EgoTracks，一个新的数据集，用于长期以自我为中心的视觉对象跟踪。来自Ego4D数据集，EgoTracks对最近最先进的单目标跟踪提出了重大挑战，根据传统的跟踪指标，作者团队发现新数据集的得分比现有的流行基准更差。作者团队进一步展示了可以对STARK跟踪器进行的改进，以显着提高其在以自我为中心的数据上的性能，从而产生了作者团队称为EgoSTARK的基线模型。

## 介绍
第一人称或“自我中心”视觉旨在捕捉智能体所面临的现实世界中的感知问题;它作为高度相关的视觉领域，最近引起了强烈的兴趣，其重要应用范围从机器人到增强和混合现实等。视觉目标跟踪（VOT），长期以来一直是视觉中的一个基本问题，是许多以自我为中心的任务的核心组成部分，包括跟踪动作或活动的进展，建立包围目标对象的（重新）关联，以及预测环境的未来状态。然而，尽管VOT领域在过去十年中取得了许多重大进展，但以自我为中心的视频中的跟踪仍然未得到充分探索。这种缺乏关注在很大程度上是由于缺乏大规模的以自我为中心的跟踪数据集进行训练和评估。虽然社区近年来提出了许多流行的跟踪数据集，包括OTB，TrackingNet，GOT-10k和LaSOT等，但作者团队发现最先进的跟踪器在这些基准上实现的强大性能并不能很好地转化为以自我为中心的视频，因此建立了对这种跟踪数据集的强烈需求。

作者团队将这种问题归因于自我中心视图的许多独特属性上，与以前第三人称视角的数据集相比。与人为有意之的“framed”视频相反，以自我为中心的视频通常是未经规划和处理的，这意味着它们往往会捕捉到活动、对象或地点之间的许多注意力转移。由于第一人称视角，相机佩戴者的大幅度头部运动通常会导致物体反复离开和重新进入视野;类似地，对物体的人为手部操作会导致频繁的遮挡，尺度和姿势的快速变化以及状态或外观的潜在变化。此外，以自我为中心的视频往往很长（有时代表相机佩戴者或独立个体的整个生活），这意味着上述遮挡和变换的数量相似。这些特征都使得在以自我为中心的视图中跟踪对象比在先前数据集中通常考虑的场景更加困难，并且它们的缺失代表了评估盲点。

头部运动，移动，手部遮挡，和短暂的时序长度都导致多种挑战。首先，频繁的对象消失和再现导致自我中心跟踪内的重新检测的问题变得特别关键。许多以前的跟踪数据集主要集中在第三人称视频中的短期跟踪，由于目标对象消失的数量和长度较低，因此评估长期自我中心跟踪的许多挑战的能力有限。因此，强大的性能依赖于鲁棒的重新检测，导致许多近几年内的短期跟踪算法都忽略它，而是预测每帧的边界框，这可能导致误报或跟踪错误的对象。此外，短期第三人称视频的特性也导致了依赖于运动和外观的渐变的设计。以前的短期跟踪算法所做的许多运动，上下文和规模先验无法转移到以自我为中心的视频。 很明显，重新检测，遮挡和长时间跟踪均在VOT中被认为是难以完善和处理的，这也是这几年跟踪强调的几个方面。作者团队认为，以自我为中心的视频为这些挑战提供了一个自然的来源，同时也代表了一个高度影响力的跟踪应用方向，因此这些都是一个个的重要的机遇。因此，作者团队提出了EgoTracks：一个大规模的长期以自我为中心的视觉对象跟踪数据集，用于训练和评估长期跟踪器。为了寻求现实的挑战，作者团队从Ego4D中获取视频，这是一个大规模的数据集，由日常生活活动的视频组成。它是一个大规模的数据集，用于评估SOT模型的跟踪和重新检测能力，其中包括来自5708个平均6分钟视频的超过22，028个轨迹。下面这个图是目标对象在视频中频繁出现和消失的体现，如下：
  ![fig1](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_fig1.jpg)
  来自EgoTracks数据集的视频，当目标（喷灯）可见时，黄色片段标记。请注意，在8分钟的视频中，物体频繁消失和再现，并且长时间缺席，需要重新检测以准确跟踪而不会出现误报。视频的自我中心性质包括摄像机佩戴者与对象交互（occurrence2），导致显著的手部遮挡和姿势的强烈变化。

作者团队针对新数据集及其相对于先前基准的新特征进行了彻底的分析，证明了它的难度和进一步研究的必要性，以开发能够处理长期自我中心视觉的跟踪器。作者团队的实验揭示了剩余的开放性问题和见解，对有前途的方向，以自我为中心的跟踪。利用这些知识，作者团队提出了多个简单而有效的改变，如调整时空先验，自我中心的数据微调，并结合多种模板。作者团队将这些策略应用于最先进的（SOTA）STARK跟踪器，训练一个致力于长期自我中心跟踪的强大跟踪器：EgoSTARK。作者团队希望EgoSTARK可以作为一个强大的基线，并促进未来的研究。

作者团队的贡献如下：
  - 作者团队提出了EgoTracks，第一个大规模的长期对象跟踪数据集与不同的自我中心的情况。作者团队分析了它的独特性，在评估跟踪器的重检测性能。
  - 作者团队进行了全面的实验，以了解许多最先进的跟踪器在EgoTracks验证集上的性能，并观察到由于现有第三人称数据集的偏见和评估盲点，它们往往会挣扎。
  - 作者团队进行了分析，是什么因素可以使一个很好的跟踪面向长时性质的自我中心的tracker。将这些学习应用于STARK跟踪器，作者团队产生了一个强大的基线，作者团队称之为EgoSTARK，它在EgoTracks上实现了显着的改进（+15% F-score）。

## 相关工作
### 目标跟踪数据集
视觉目标跟踪研究视频中目标的时空联合定位。从视频和预定义的分类，多目标跟踪（MOT）模型同时检测，识别和跟踪多个对象。例如，MOT数据集跟踪人类，KITTI数据集跟踪行人和汽车，TAO跟踪833个类别的大型分类。相比之下，单目标跟踪（SOT）经由所提供的对象的初始模板跟踪单个对象，而不进行任何检测或识别。因此，SOT通常是无分类法的，并对通用意义上的目标对象进行操作。当前的社区已经构建了多个流行的基准来研究这个问题，包括OTB-50/100，UAV-123，NfS，TC-128，NUS-PRO，GOT-10k，VOT-series、LaSOT和TrackingNet等。虽然这些SOT数据集主要由短视频（例如几秒钟）组成，但长期跟踪越来越受到关注。在较长的视频（几分钟或更长时间）中跟踪对象会带来独特的挑战，例如显著的变换、位移、消失和再现。除了在可见时定位对象之外，模型还必须在对象不存在时不产生框，然后在对象重新出现时重新定位同一对象。OxUvA是最早对较长视频（平均2分钟）进行基准测试的视频之一，拥有366个仅限评估的视频。LaSOT 将其扩展到1400个视频，具有更频繁的对象再现。同时，VOT-LT则包括50个故意选择的视频中频繁的目标物体消失和再现。作者团队的EgoTracks专注于长期的SOT，并呈现出多个关键和独特的属性：
  - 1）显著更大的规模，平均6分钟的5708个视频; 
  - 2）更频繁的消失和再现（平均17.7次）发生在自然，真实世界的场景; 
  - 3）数据来源于自我中心的视频拍摄在野外，涉及独特的挑战性的情况下，如大相机运动，不同的角度变化，手物体的相互作用，和频繁的闭塞。
### 单目标跟踪算法
许多现代方法使用卷积神经网络（CNN），无论是Siamese网络还是基于相关滤波器的架构。随着最近在分类和检测等视觉任务中大获成功的基于Transformer架构的跟踪算法也变得流行起来。例如，TransT使用基于注意力的特征融合来关联模板特征和搜索图像的特征。最近，一些工作利用Transformer作为直接预测器来实现新的SOTA，例如STARK，ToMP和SBT。这些模型将来自ResNet编码器的帧特征token化，并使用Transformer来预测边界框和具有特征标记的目标存在得分。这些方法通常是在短期SOT数据集上开发的，并假设目标对象以最小的遮挡保持在视场中。另一方面，长期跟踪器被设计用于实现在目标再现时重新检测到的问题。这些方法旨在了解潜在的目标对象消失，搜索整个图像以寻找其再现的区域。

### 第一视角中的目标跟踪
过去几十年中引入了多个以自我为中心的视频数据集，带来了许多有趣的挑战，其中许多需要跨帧关联目标对象：活动识别，预期，视频摘要、人与物体交互、情景记忆、视觉查询和相机佩戴者姿势推断。 为了应对这些挑战，许多方法都利用了跟踪，但很少有工作专门致力于解决这个基本问题。  但是有很多工作已经开始认识到自我为中心在跟踪中的挑战，然而，很多的工作之间的主要区别在于数据集的规模：例如TREK包含 150 个仅用于评估的轨道，而 EgoTracks 则大 100 倍，包含 20k 个带有训练和评估分割的轨道。 此外，虽然过去的工作是从以厨房为主的 EPICKITCHEN 中获取视频，但 EgoTracks 从 Ego4D 中获取视频，后者具有更多样化的场景。  EgoTracks 提供了一个独特的大规模测试平台，用于开发专用于以自我为中心的视频的跟踪方法； 作者团队改进的基线 EgoSTARK 还可以作为潜在的即插即用模块来解决需要对象关联的其他任务。下面的表格就是一个不同规模VOT数据集的比较：

  VOT数据集比较。EgoTracks除了比以前的数据集规模更大之外，其捕获的场景对 SOTA 跟踪器来说是一个更加艰巨的挑战，这表明跟踪方法还有改进的空间。![egotracks_tab1](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab1.jpg)

在以自我为中心的视频理解中，Ego4D和 EPIC-KITCHENS VISOR密切相关。Ego4D 包含最大的野外以自我为中心的视频集合；EgoTracks 在 Ego4D 的子集上进行注释。 此外，Ego4D 提出了许多新颖的任务，例如情景记忆任务，其中跟踪被确定为核心组件。而对于VISOR，使用实例分割掩码注释来自 EPIC-KITCHENS的短期（平均 12 秒）视频。 作者团队相信 EgoTracks 提供了与 EPIC-VISOR 互补的多种独特价值：长期跟踪（6 分钟 vs. 12 秒）、规模显着扩大（5708 个视频片段 vs. 158 个）以及更多样化的视频源（80+ 场景 vs. 158 个场景）。下面是两者的一个比较：

  ![egotracks_fig4](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_fig4.jpg)左边是场景，右边是目标对象

## EgoTracks 数据集
EgoTracks：一个大规模长期以自我为中心的单目标跟踪数据集，由 5708 个视频中的总共 22028 个轨迹组成。作者团队遵循与 Ego4D Visual Queries (VQ) 2D 基准相同的数据注释。

  - 数据划分。作者团队对训练集、验证集和测试集遵循与VQ2D相同的数据分割。 视频数量和轨道数量的确切数字可能与其不同，因为注释者被指示忽略由于视频分辨率差、无法理解等原因而可能不知道如何注释的视频和对象 要跟踪什么对象等等。![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab11.jpg)
  - 目标尺寸大小分布。下面的图显示了 EgoTracks 中目标对象大小的分布。 为了解释 Ego4D 中视频分辨率的差异，作者团队使用标准化对象大小而不是绝对像素大小来绘制分布。 标准化对象尺寸定义为对象尺寸除以图像分辨率：$\sqrt{obj_w\times obj_h}/\sqrt{img_w\times img_h}.$ ![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_fig6.jpg)

### Ego4D 视觉查询基准
Ego4D这个数据集是由参与者处于野外拍摄的长达 3670 小时的多样化日常生活活动组成；这些视频已删除个人身份信息，并筛选了攻击性内容。该数据集附带多个基准测试，但与作者团队的目的最相关的任务是情景记忆的VQ2D任务：给定一个以自我为中心的视频和一个对象的裁剪图像，目标是定位该对象最后一次在场景中出现的时间和位置。作为连续帧中的一系列 2D 边界框。该任务与长期跟踪密切相关：在给定视觉模板的情况下找到视频中的目标对象，这一任务目的均是相同的。长时视频跟踪中依然面临重新检测的问题，其次，Ego4D的基线算法很大程度上依赖于跟踪方法，siamrcnn和kys这种用于全局和局部跟踪的方法。

<b>缺陷</b>。虽然任务高度相关，但 VQ 数据集并不是适合长期跟踪。具体来说，VQ注释指南大致如下：1）识别在视频中多次出现的三个不同对象； 2）为每个对象注释一个查询模板，该模板应包含整个对象，没有任何运动模糊； 3）注释在时间上远离模板的对象的出现。因此，随着时间的推移，这些注释并不详尽（它们非常稀疏），限制了它们对跟踪的适用性。 另一方面，选择标准会产生一组强大的候选对象，作者团队利用它们来构建 EgoTracks。

### VQ注释用于长时跟踪
因此，作者团队从 VQ 视觉裁剪和响应轨迹开始，要求注释者首先识别由视觉裁剪、响应轨迹和对象名称表示的对象。 从视频开始，作者团队指示注释者每次出现时在对象周围绘制一个边界框。 由于注释者必须完整地浏览每个视频，其中平均包含约 1800 帧，每秒 5 帧 (FPS)，因此此注释任务是劳动密集型的，每个轨迹大约需要 1 到 2 小时。 此注释的一个重要方面是其详尽性：整个视频针对目标对象进行了密集注释，任何没有边界框的帧都被视为负片或者负示例。能够拒绝负面示例是在现实环境中重新检测的重要组成部分，因为误报可能会像假阴性样本那样影响某些应用程序。

<b>质量保证</b>。所有轨迹在初始注释后均由专家注释2者进行质量检查。为了衡量注释质量，作者团队对验证集的子集采用多重审查。 要求三名独立审稿人对同一个视频进行注释。作者团队发现这些独立注释之间的重叠度很高（> 0.88 IoU）。 此外，由于 EgoTracks 专注于重新检测，因此作者团队检查对象存在的时间重叠，并发现它在注释器之间是一致的。总的来说，整个注释工作大约需要 86,000 个工时。

### 轨迹属性
除了边界框注释之外，作者团队还标记某些相关属性，以允许不同的训练策略或更深入地分析验证集性能。作者团队对每次出现的以下三个属性进行注释，is_active, is_transformed和is_recognizable。
  - is_active:在Ego4D中，相机佩戴者经常用手与相关目标对象进行交互。 由于频繁的遮挡和快速的姿态变化，处于被处理状态的物体对跟踪算法提出了挑战。
  - is_transformed: Ego4D中的目标对象可能会发生变换，例如变形和状态变化。 这种情况需要能够快速适应具有新外观的跟踪对象。
  - is_recognizable:由于遮挡、运动模糊、尺度或其他条件，如果没有额外的上下文，Ego4D 中的某些对象可能极难识别。因此，作者团队仅根据其外观来注释对象是否可识别，而不使用额外的上下文信息（例如其他视频帧）。

如下图和下表所示：

  ![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_fig3.jpg)这个图展示了EgoTracks tracklet 属性示例。左：工作台上的微量移液器（上）与实际使用的微量移液器（下）。 中：油漆罐（上）被打开（下）。右图：由于距离和运动模糊（底部），喷灯（顶部）需要其他帧的上下文来识别。
  ![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab2.jpg)这个表展示的是跟踪训练/验证集中的属性。

### 数据多样性
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_fig4.jpg)
上图显示了EgoTracks在视频包含的动作和对象轨迹名称方面的多样性。  Ego4D视频描绘了在世界各地拍摄的各种野外活动（图左）。因此，它们代表的场景比其他以自我为中心的数据集更加多样化，因此需要模型学习更通用的表示。对于要跟踪的对象没有固定的分类法。相反，注释者被要求选择“有趣”的对象，这些对象是自由命名的，从而导致了相当大的对象多样性。 作者团队从这些对象名称中提取了名词，产生了大约 1000 多个唯一名称。图右显示了EgoTracks 中跟踪的各种对象。

## SOTA跟踪器的分析
作者团队比较了 EgoTracks 验证集上几种现成跟踪模型的性能。 将STARK确定为性能最佳的一种，作者团队使用STARK在不同设置下进行进一步的消融研究，以进一步了解其行为。
### 评估协议和指标
<b>评估协议</b>。 作者团队介绍了 EgoTracks 的几种评估协议，包括初始模板、评估帧和跟踪器运行的时间方向的不同组合。对于初始模板，作者团队考虑两种选择：
  - 视觉裁剪模板（VCT）：视觉裁剪图像被专门选择为目标的高质量视图，并作为作者团队的注释者在整个视频中识别对象的参考。因此，它们是初始化跟踪器的理想选择。
  - 发生第一帧模板(OFFT)：跟踪器用每次出现的第一帧进行初始化（参见下面的OO）。虽然这可能会导致对象视图质量较低，但与后续帧的时间接近意味着它在外观上可能更接近。

请注意，作者团队从任何评估指标的计算中排除模板框架。作者团队还考虑了评估帧和时间方向的几种选择：
  - 视频开始向前（VS）：从第一帧开始，跟踪器按照随机的顺序对视频中的每一帧进行评估。 这代表跟踪器在长视频中跟踪对象的能力。  
  - 视觉裁剪向前/向后(VC)：跟踪器在视频上运行两次，一次从视觉裁剪帧开始向前和时间运行，第二次向后运行。 这代表了覆盖视频中每一帧的另一种方法，但 VCT 初始化和跟踪器遇到的第一帧之间具有更接近的视觉相似性。
  - 仅向前出现的情况(OO)：跟踪器仅在目标可见的帧上进行评估，而目标不可见时不进行评估。这简化了跟踪任务，并使作者团队能够将重新检测的挑战与简单地在以自我为中心的剪辑中跟踪的挑战分开。
作者团队通过连接适当的描述符来指定协议。 作者团队在实验中主要考虑 VCT−→ VS、VCT←→ VC、VCT−→ OO 和 OFFT−→ OO。如下表所示：
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab3.jpg)

<b>指标</b>。 作者团队在目标跟踪中采用常见的指标，包括F-score、精度和召回率；跟踪算法主要根据 F-score进行排名。 作者团队还考虑平均重叠（AO）、成功率、精度和归一化精度作为短期跟踪指标。

### SOTA跟踪器在EgoTracks上的表现
作者团队在EgoTracks 上选定跟踪器，并在VCT→VS 评估协议上进行评估比较。考虑到跟踪算法的广度，作者团队的目标不是详尽无遗，而是选择不同跟踪原理的高性能代表。 KYS和 DiMP是两种维持在线目标表示的短期跟踪算法。ToMP、STARK 和 MixFormer是基于 Transformer 的 SOTA 短期跟踪器的三个示例。GlobalTrack是一个全局跟踪器，它搜索整个搜索图像以进行重新检测。LTMU是一种高性能长期跟踪器，结合了全局跟踪器（GlobalTrack）和局部跟踪器（DiMP）。SiamR-CNN利用动态规划来模拟长期的完整历史路径。下表总结了这些跟踪器在EgoTracks上的性能。
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab4.jpg)
该表4中的 AO 相当于概率阈值 0 时的召回率。定性结果如下图所示。
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_fig5.jpg)
作者团队强调几个发现。首先，大多数短期跟踪器的目标存在分数不是很有用，从 KYS（12.5）、DiMP（13.91）和 ToMP（19.63）的低精度可以看出，而像 GlobalTrack 这样的长期跟踪器，DiMP_LTMU 和 Siam R-CNN 实现了更高的精度，分别为 31.28、37.28 和 52.8。 这是预料之中的，因为长期跟踪器的设计更加注重高重新检测精度，尽管显然仍有改进的空间。STARK 达到了第二高的精度（34.70），这是一个例外，因为它有第二个训练阶段来教模型对物体是否存在进行分类。 其次，MixFormer 和 STARK 等最近的作品比以前的短期追踪器取得了更好的 F-score。 这可能归因于训练策略、更多数据和基于 Transformer 的架构的进步。 令人惊讶的是，作者团队发现最近的 MixFormer并没有优于 STARK，尽管在其训练数据集上实现了新的 SOTA。 这凸显了泛化的潜在困难。作者团队还包括使用检测跟踪原理的结果：检测器产生 100 个边界框，作者团队使用框特征的余弦相似度选择最佳边界框。作者团队观察到，在 COCO 上训练的开放世界检测器 GGN通过预言匹配具有相当好的泛化能力，达到 75.92 AO。 然而，关联问题非常具有挑战性，其 AO 降至 15.19。 实施细节如下：检测跟踪框架包含两个模块： 
  - 生成边界框建议的检测器。 每个建议的边界框都包含一个可能对应于跟踪目标的实例。 由于SOT专注于通用对象，因此该检测器模块应尽可能通用，而不限于特定类型的对象。
  - 关联模块，将建议的边界框链接到模板。该模块从检测器获取建议并选择最有可能对应于模板对象的建议。
详细的介绍如下：
#### 检测器模块
作者团队对两个现成的模型进行了基准测试：基线 MaskR-CNN 和开放世界实例分割模型 GGN。 两个模型都在具有 80 个对象类别的 COCO2017数据集上进行训练。这些模型以与类无关的方式（对象与背景）进行训练，因为它可以更好地概括通用对象。 作者团队选择开放世界模型，因为作者团队想要一个能够检测物体（无论其类别如何）的检测器，这是开放世界检测器的主要优势之一。 GGN 使用与Mask R-CNN 类似的架构，但通过专用于开放世界的新训练策略在多个基准上实现了SOTA。 作者团队使用来自bbox头部的边界框作为边界框建议。为了评估检测器，作者团队使用 Oracle 来选择与真实值匹配的最佳框（通过与真实值的最大重叠）。两种模型都达到了合理的性能，如下面所示。
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab12.jpg)
AO@100 远高于跟踪基线。 开放世界的 GGN 模型在少量提案 (AO@10) 和大量提案 (AO@100) 上都取得了更强的性能。 这表明检测通用对象（无论其类别如何）的能力确实是 EgoTracks的一个重要属性。 此外，这也证明 EgoTracks 包含许多不属于流行的 COCO 分类法的对象，因为 Mask R-CNN 的表现明显较差。
#### 关联模块
作者团队使用边界框（实例）嵌入从建议的边界框中选择作为跟踪输出。具体来说，作者团队根据建议的边界框和模板图像计算裁剪图像上的嵌入特征。 然后作者团队计算模板特征和所有边界框特征之间的余弦相似度。作者团队选择余弦相似度最高的边界框作为跟踪输出，余弦相似度作为跟踪置信度输出。作者团队考虑两种模型来提取嵌入。
  - (1) 作者团队使用在 ImageNet 上训练的 ResNet-50 作为基线。
  - (2) 为了加强对不同实例的区分，作者团队使用经过附加实例区分任务 InstEmb 训练的 ResNet-50 模型。 

除了与基线模型类似的标准交叉熵损失之外，作者团队通过跨实例（相同类别和不同类别）进行对比来添加监督对比损失。 为了生成更多实例示例，作者团队通过基于边界框注释裁剪图像来将 Objects365添加到训练数据中。作者团队使用 GGN 模型中的前 100 个边界框。 作者团队在下表中总结了这两个模型。作者团队观察到使用基线 ImageNet 特征并不能在检测结果和模板对象之间产生良好的关联。 InstEmb 模型实现了更强的性能，显示了从额外的实例对比损失中进行实例区分的重要性。 尽管其性能更强，但与大多数跟踪方法相比仍存在显着差距。 这表明时间关联问题在 EgoTracks 中非常具有挑战性。
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab567.jpg)
### 重新检测和不同视角具有挑战性
作者团队根据替代的评估协议进行了额外的 EgoTracks 实验，以进一步了解跟踪器性能。如表五所示。
为了将重新检测问题与 EgoTracks 的其他以自我为中心的方面分离，作者团队使用 OFFT−−→ OO 协议进行评估，该协议忽略视频的负帧，从而避免了重新检测的需要。 毫不奇怪，所有跟踪器的表现都明显更好，这凸显了 EgoTracks 中重新检测的挑战性。 作者团队还在 VCT←→ VC 设置中进行实验，其中初始模板在时间上与第一个跟踪帧相邻。在这里，作者团队看到与 VCT→VS协议相比，AO、F-score、精确度和召回率提高了3-4%，这说明像 STARK 这样的跟踪器被设计为期望外观逐渐转变。 这两个实验都表明，重新检测问题是跟踪的重大挑战，并且需要更好的长期基准。

### 属性捕获难以跟踪的场景
作者团队使用先前提到的验证集 tracklet 属性注释来进一步了解评估集的性能。 对于每个属性，作者团队将 tracklet 分为两组，对应于属性 true 和 false。 然后，作者团队使用标准 STARK 跟踪器 并使用表 6 中的 OFFT−−→ OO 评估协议报告每组 tracklet 的 AO。 正如所料，作者团队发现当目标被用户主动使用或在在转变过程中，AO 往往较低，大约 6%，可能是由于遮挡或外观变化。 此外，当图像中的对象由于遮挡、模糊、缩放或其他条件而难以识别时，STARK往往会遇到困难。
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab890.jpg)
### 在 EPIC-VISOR 上评估 EgoSTARK
为了证明通过 EgoTracks 训练的以自我为中心的对象跟踪泛化，作者团队对 EPIC-VISOR 数据集进行了进一步的定量评估，这是一个专注于厨房场景中的活动对象的以自我为中心的数据集。作者团队观察到，使用 EgoTracks 进行训练使得 Siam R-CNN 和 STARK 在 EPIC-VISOR 上的跟踪方面都有所改进（表 8），证明了 EgoTracks 作为大规模预训练数据集的价值。

## 以自我为中心的跟踪设计考虑因素
观察到现有的跟踪器在 EgoTracks 上表现不佳，作者团队对以自我为中心的跟踪的先验和其他设计选择进行了系统的探索。 尽管不是专门为长期跟踪而设计的。作者团队专注于对EgoSTARK这个跟踪器进行额外的分析，建议改进以自我为中心的表现。
### 以自我为中心的微调至关重要
作者团队首先演示了在第三人称视频上训练的各种跟踪器如何从 EgoTracks 上的微调中显著受益。 如表 7 所示，所有方法的 F-score均获得了6% - 10% 的改进。 此外，如表 9 所示，对 VQ 响应轨迹子集进行微调将 F-score从 30.48% 提高到 33.53%，而使用完整的 EgoTracks 注释进一步将 F-score提高 4.67% 到 38.2%。 这表明：1）使用自我中心数据进行微调有助于缩小外中心-自我中心域差距；  2）完整的 EgoTracks 训练提供了进一步的收益，显示了作者团队训练集的价值。

### 第三人称时空先验失败
现代 SOT 发现关于对象运动、外观和周围环境的某些假设对过去的数据集很有帮助，但其中一些设计选择很难转化为长期以自我为中心的视频。
- Search window size.
一个例子是局部搜索。许多跟踪器假设被跟踪的对象出现在其先前位置的一定范围内。 因此，为了提高效率，这些方法通常在下一帧的局部窗口内进行搜索。这在高FPS、运动相对较慢的平滑视频中是合理的，通常在以前的短期跟踪数据中，但在以自我为中心的视频中，对象的像素坐标可以快速变化（频繁的大幅头部运动），重新检测成为关键问题。 因此，作者团队尝试扩大搜索区域，超出过去方法中常见的范围。当作者团队将搜索大小从 320 扩大到 800时，作者团队看到了显著的改进（表 9）：STARK 能够定位之前由于快速移动而位于搜索窗口之外的对象。
- 多尺度增强。以自我为中心的视频的特征也会影响目标尺度的常见 SOT 假设。许多跟踪器的训练假设是物体的尺度与模板图像以及相邻帧之间一致。 然而，大的以自我为中心的相机运动、运动以及与物体的手部交互（例如，将物体带到一个人的脸上，如吃饭时）可以转化为物体快速经历巨大的尺度变化。 因此，作者团队建议在训练期间添加尺度增强，按 s ∈ [0.5, 1.5] 因子随机调整搜索图像的大小。 虽然简单，但作者团队发现这极大地提高了 EgoTracks 的性能，将 STARK 的 AO 提高了近 10%，F-score提高了 8% 以上（表 9）。
- 上下文比率。 过去的 SOT 工作发现，包含一些背景有助于模板图像特征提取，其中常见的是物体大小的两倍。作者团队尝试不同的上下文比例，看看这个经验法则是否适用于以自我为中心的视频。 由于局部窗口假设，模板和搜索图像的大小是相关的：搜索图像大小（SIS）/搜索区域比率（SRR）=模板图像大小/上下文比率（CR）=对象比例。模板图像尺寸设置为固定尺寸128×128。在改变上下文比例时，作者团队仔细控制其他参数以进行公平比较。 结果如表 10 所示。在所有三个参数 - CR、SRR 和 SIS 中，搜索区域大小（由 SRR 和 SIS 确定）对 F-score的影响最大。 这是预料之中的，因为会频繁地重新检测，这需要跟踪器在更大的区域中搜索对象，而不仅仅是在常用的局部窗口内。改变 CR 会产生不同的结果，因此作者团队坚持使用 CR 为 2 的常见做法。

## 结论
作者团队推出了 EgoTracks，这是第一个用于在不同场景中进行长期以自我为中心的视觉对象跟踪的大型数据集。 作者团队进行了大量的实验，以了解最先进的跟踪器在这个新数据集上的性能，并发现它们表现相当困难，部分原因可能是过度拟合现有基准的一些更简单的特征。 因此，作者团队提出了针对自我中心领域的一些调整，从而形成了一个强大的基线，作者团队称之为 Ego-STARK，它极大地提高了 EgoTracks 上的性能。 通过公开发布该数据集，作者团队希望鼓励长期跟踪领域的进步，并吸引更多人关注长期和以自我为中心的视频的挑战。挑战和未来方向。 根据表 4 和表 5 中的实验，作者团队发现重新检测是长期跟踪的一个关键挑战，特别是在以自我为中心的视频中，其中对象经常进出视野，或者暴露于高度运动模糊。 作者团队看到了一些有希望的未来方向：
  - 更强的特征：Oracle 和InstEmb 检测跟踪变体之间的性能差距相当大（表4），说明了区分性功能不足的影响。 更好的特征，例如几何关键点、光流或长期轨迹，将允许改进对象的关联，从而实现重新检测。
  - 利用空间信号：摄像机轨迹可以为跟踪器提供强大的时空先验。例如，了解相机姿态可以帮助重新定位在视野之外位置不会改变的对象。
  - 全局、多视图对象表示：以自我为中心的视频具有不同的摄像机轨迹和捕捉与对象交互的倾向，通常比传统的第三人称跟踪数据集提供更加多样化的对象视角。在后者中，对象外观往往更加恒定，因此先前的跟踪方法通常可以使用单个图像模板。以自我为中心的视频的上述特征需要更高的鲁棒性，而像 EgoTracks 这样具有挑战性的以自我为中心的跟踪数据集代表了开发以在线方式学习的更全局、视图变化的对象表示的机会。如下是详细的解释：基于 Transformer 的架构可以对任意长度的输入进行编码，从而可以轻松地使用任意数量的模板中的特征。 最初的 STARK 设计编码两个模板：初始化模板和单个动态更新模板。 一个自然的扩展是包含更多模板，这可能会使转换器暴露于对象的不同视图（特别是在以自我为中心的视频中相关），尽管低质量的视图可能会损害性能。 什么是正确的权衡？ 作者团队对基本 STARK 模型使用不同数量的模板进行实验。 受用户可以从不同角度拍摄物体短视频的潜在应用的推动，作者团队通过合并视觉裁剪作为模板出现的同一事件中的附加帧，将单个视觉裁剪扩展到模板的视觉剪辑 。 作者团队采用简单的模板采样方法：从视觉裁剪的出现中均匀采样 3、5、7 或 9 个模板。 对视频进行时间上的统一采样可能是一种简单而有效的启发式方法，可以从事件中收集不同的视图。 作者团队在表 15 中总结了结果。虽然作者团队观察到使用最多 5 个模板的所有指标都有所改善，但随着模板数量的增加，性能会下降。 作者团队假设增加模板的数量确实会增加 STARK 可用于跟踪的知识，但在某一点之后，它可能会稀释模板中的信息并使 Transformer 难以合成。 这凸显了模板选择和多视图融合机制的重要性，从而激发了有希望的方向。
![](https://cyfedu-dlut.github.io/PersonalWeb/images/egotracks_tab15.jpg)



